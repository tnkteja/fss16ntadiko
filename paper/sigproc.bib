@article{Nguyen2012,
abstract = {Detecting duplicate bug reports helps reduce triaging efforts and save time for developers in fixing the same issues. Among several automated detection approaches, text-based information retrieval (IR) approaches have been shown to outperform others in term of both accuracy and time efficiency. However, those IR-based approaches do not detect well the duplicate reports on the same technical issues written in different descriptive terms. This paper introduces DBTM, a duplicate bug report detection approach that takes advantage of both IR-based features and topic-based features. DBTM models a bug report as a textual document describing certain technical issue(s), and models duplicate bug reports as the ones about the same technical issue(s). Trained with historical data including identified duplicate reports, it is able to learn the sets of different terms describing the same technical issues and to detect other not-yet-identified duplicate ones. Our empirical evaluation on real-world systems shows that DBTM improves the state-of-the-art approaches by up to 20{\%} in accuracy.},
annote = {Keywords and Definitions:

Technical Issue: The actual root cause of a bug.
Technical topics: Perspective based reports with different description of the root cause, suggestions, tips for the fixes e.t.c with characteristics of latent, semantic features},
author = {Nguyen, Anh Tuan and Nguyen, Tung Thanh Tien N. and Lo, David and Sun, Chengnian},
doi = {10.1145/2351676.2351687},
file = {:G$\backslash$:/NCSU/CSC 791/Reading Part -1/06494907 - Duplicate Bug Report Detection with a Combination of Information Retrieval and Topic Modelling.pdf:pdf;:G$\backslash$:/NCSU/CSC 791/Reading Part -1/Summary.docx:docx},
isbn = {9781450312042},
journal = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering - ASE 2012},
keywords = {Duplicate Bug Reports,Information Retrieval,Topi,duplicate bug reports,information retrieval,topic model},
pages = {70},
title = {{Duplicate bug report detection with a combination of information retrieval and topic modeling}},
url = {http://dl.acm.org/citation.cfm?doid=2351676.2351687},
year = {2012}
}
@article{Sun2011,
abstract = {In a bug tracking system, different testers or users may submit multiple reports on the same bugs, referred to as duplicates, which may cost extra maintenance efforts in triaging and fixing bugs. In order to identify such duplicates accurately, in this paper we propose a retrieval function (REP) to measure the similarity between two bug reports. It fully utilizes the information available in a bug report including not only the similarity of textual content in summary and description fields, but also similarity of non-textual fields such as product, component, version, etc. For more accurate measurement of textual similarity, we extend BM25F - an effective similarity formula in information retrieval community, specially for duplicate report retrieval. Lastly we use a two-round stochastic gradient descent to automatically optimize REP for specific bug repositories in a supervised learning manner. We have validated our technique on three large software bug repositories from Mozilla, Eclipse and OpenOffice. The experiments show 10 -- 27{\%} relative improvement in recall rate and 17 -- 23{\%} relative improvement in mean average precision over our previous model. We also applied our technique to a very large dataset consisting of 209,058 reports from Eclipse, resulting in a recall rate of 37 -- 71{\%} and mean average precision of 47{\%}.},
author = {Sun, Chengnian and Lo, David and Khoo, Siau Cheng and Jiang, Jing},
doi = {10.1109/ASE.2011.6100061},
file = {:C$\backslash$:/Users/tnkte/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun et al. - 2011 - Towards more accurate retrieval of duplicate bug reports.pdf:pdf},
isbn = {9781457716393},
issn = {1938-4300},
journal = {2011 26th IEEE/ACM International Conference on Automated Software Engineering, ASE 2011, Proceedings},
mendeley-groups = {CSC 791},
pages = {253--262},
title = {{Towards more accurate retrieval of duplicate bug reports}},
year = {2011}
}
@article{Kautz,
author = {Kautz, Henry and Selman, Bart and Jiang, Yueyen},
file = {:C$\backslash$:/Users/tnkte/Desktop/10.1.1.21.2218.pdf:pdf},
mendeley-groups = {CSC 791},
pages = {1--14},
title = {{A General Stochastic Approach to Solving Problems with Hard and Soft Constraints}},
volume = {00}
}
@article{Tian2013,
author = {Tian, Yuan and Lo, David and Sun, Chengnian},
doi = {10.1109/ICSM.2013.31},
file = {:/desktop-o023an0/g/NCSU/CSC 791/Reading Part -1/06676891.pdf:pdf},
mendeley-groups = {CSC 791},
title = {{DRONE : Predicting Priority of Reported Bugs by Multi-Factor Analysis}},
year = {2013}
}

@article{Wang2016,
author = {Wang, Song and Liu, Taiyue and Tan, Lin},
file = {:/desktop-o023an0/g/NCSU/CSC 791/Reading Part -1/eight - p297-wang.pdf:pdf},
isbn = {9781450339001},
keywords = {defect prediction, feature generation, deep learning},
mendeley-groups = {CSC 791},
title = {{Automatically Learning Semantic Features for Defect Prediction}},
year = {2016}
}
@article{Alipour2013,
author = {Alipour, Anahita and Hindle, Abram and Stroulia, Eleni},
file = {:/desktop-o023an0/g/NCSU/CSC 791/Reading Part -1/five - p183-alipour.pdf:pdf},
isbn = {9781467329361},
mendeley-groups = {CSC 791},
pages = {183--192},
title = {{A Contextual Approach towards More Accurate Duplicate Bug Report Detection}},
year = {2013}
}
@article{Sun2010,
author = {Sun, Chengnian and Lo, David and Wang, Xiaoyin and Jiang, Jing and Khoo, Siau-cheng},
file = {:/desktop-o023an0/g/NCSU/CSC 791/Reading Part -1/four - p45-sun - A Discriminative Model Approach for Accurate Duplicate.pdf:pdf},
isbn = {9781605587196},
keywords = {[Electronic Manuscript]},
mendeley-groups = {CSC 791},
pages = {45--54},
title = {{A Discriminative Model Approach for Accurate Duplicate Bug Report Retrieval}},
year = {2010}
}
@article{Nguyen,
author = {Nguyen, Anh Tuan and Lo, David},
file = {:/desktop-o023an0/g/NCSU/CSC 791/Reading Part -1/one - 06494907 - Duplicate Bug Report Detection with a Combination of Information Retrieval and Topic Modelling.pdf:pdf},
isbn = {9781450312042},
keywords = {Duplicate Bug Reports, Information Retrieval, Topic Model,duplicate bug reports,information retrieval,topic model},
mendeley-groups = {CSC 791},
pages = {70--79},
title = {{Duplicate Bug Report Detection with a Combination of Information Retrieval and Topic Modeling}}
}
@article{Moran,
author = {Moran, Kevin and Linares-v{\'{a}}squez, Mario and Bernal-c{\'{a}}rdenas, Carlos and Poshyvanyk, Denys and William, College},
file = {:/desktop-o023an0/g/NCSU/CSC 791/Reading Part -1/p673-moran.pdf:pdf},
isbn = {9781450336758},
keywords = {all or part of,android,auto-completion,bug reports,classroom use is granted,copies are not made,or,or distributed,or hard copies of,permission to make digital,reproduction steps,this work for personal,without fee provided that},
mendeley-groups = {CSC 791},
pages = {673--686},
title = {{Auto-completing Bug Reports for Android Applications Categories and Subject Descriptors}}
}
@article{Xia2015,
author = {Xia, Xin and Lo, David and Shihab, Emad and Wang, Xinyu and Yang, Xiaohu},
doi = {10.1016/j.infsof.2014.12.006},
file = {:/desktop-o023an0/g/NCSU/CSC 791/Reading Part -1/seven - 1-s2.0-S0950584914002602-main.pdf:pdf},
issn = {0950-5849},
journal = {Information and Software Technology},
mendeley-groups = {CSC 791},
pages = {93--106},
publisher = {Elsevier B.V.},
title = {{ELBlocker : Predicting blocking bugs with ensemble imbalance learning}},
url = {http://dx.doi.org/10.1016/j.infsof.2014.12.006},
volume = {61},
year = {2015}
}
@article{Wong,
author = {Wong, Chu-pan and Xiong, Yingfei and Zhang, Hongyu and Hao, Dan and Zhang, Lu and Mei, Hong},
file = {:/desktop-o023an0/g/NCSU/CSC 791/Reading Part -1/six - download.pdf:pdf},
keywords = {bug report,fault localization,feature location,in-},
mendeley-groups = {CSC 791},
title = {{Boosting Bug-Report-Oriented Fault Localization with Segmentation and Stack-Trace Analysis}}
}

@article{Nguyen2011,
abstract = {Locating buggy code is a time-consuming task in software development. Given a new bug report, developers must search through a large number of files in a project to locate buggy code. We propose BugScout, an automated approach to help developers reduce such efforts by narrowing the search space of buggy files when they are assigned to address a bug report. BugScout assumes that the textual contents of a bug report and that of its corresponding source code share some technical aspects of the system which can be used for locating buggy source files given a new bug report. We develop a specialized topic model that represents those technical aspects as topics in the textual contents of bug reports and source files, and correlates bug reports and corresponding buggy files via their shared topics. Our evaluation shows that BugScout can recommend buggy files correctly up to 45{\%} of the cases with a recommended ranked list of 10 files.},
author = {Nguyen, Anh Tuan and Nguyen, Tung Thanh and Al-Kofahi, Jafar and Nguyen, Hung Viet and Nguyen, Tien N.},
doi = {10.1109/ASE.2011.6100062},
file = {:C$\backslash$:/Users/tnkte/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen et al. - 2011 - A topic-based approach for narrowing the search space of buggy files from a bug report.pdf:pdf},
isbn = {9781457716393},
issn = {1938-4300},
journal = {2011 26th IEEE/ACM International Conference on Automated Software Engineering, ASE 2011, Proceedings},
keywords = {Defect Localization,Topic Modeling},
mendeley-groups = {CSC 791,CSC 791/Reading},
pages = {263--272},
title = {{A topic-based approach for narrowing the search space of buggy files from a bug report}},
year = {2011}
}

@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M and Edu, Blei@cs Berkeley and Ng, Andrew Y and Edu, Ang@cs Stanford and Jordan, Michael I and Edu, Jordan@cs Berkeley},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
file = {:C$\backslash$:/Users/tnkte/Desktop/BleiNgJordan2003.pdf:pdf},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
mendeley-groups = {CSC 791/Reading},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
volume = {3},
year = {2003}
}


@article{Marcus2004,
abstract = {Concept location identifies parts of a software system that implement a specific concept that originates from the problem or the solution domain. Concept location is a very common software engineering activity that directly supports software maintenance and evolution tasks such as incremental change and reverse engineering. This work addresses the problem of concept location using an advanced information retrieval method, Latent Semantic Indexing (LSI). LSI is used to map concepts expressed in natural language by the programmer to the relevant parts of the source code. Results of a case study on NCSA Mosaic are presented and compared with previously published results of other static methods for concept location. View full abstract},
author = {Marcus, Andrian and Sergeyev, Andrey and Rajlieh, V{\'{a}}clav and Maletic, Jonathan I.},
doi = {10.1109/WCRE.2004.10},
file = {:C$\backslash$:/Users/tnkte/Desktop/07e1b93fb57ba017c846c858f665dda0eecd.pdf:pdf},
isbn = {0769522432},
issn = {10951350},
journal = {Proceedings - Working Conference on Reverse Engineering, WCRE},
mendeley-groups = {CSC 791/Reading},
pages = {214--223},
title = {{An information retrieval approach to concept location in source code}},
year = {2004}
}

@article{Chih-WeiHsuChih-ChungChang2008,
abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {{Chih-Wei Hsu, Chih-Chung Chang}, and Chih-Jen Lin},
doi = {10.1177/02632760022050997},
eprint = {0-387-31073-8},
file = {:C$\backslash$:/Users/tnkte/Desktop/guide.pdf:pdf},
isbn = {013805326X},
issn = {1464-410X},
journal = {BJU international},
mendeley-groups = {CSC 791/Reading},
number = {1},
pages = {1396--400},
pmid = {18190633},
title = {{A Practical Guide to Support Vector Classification}},
url = {http://www.csie.ntu.edu.tw/{~}cjlin/papers/guide/guide.pdf},
volume = {101},
year = {2008}
}

@article{Hssina2014,
abstract = {—Data mining is the useful tool to discovering the knowledge from large data. Different methods {\&} algorithms are available in data mining. Classification is most common method used for finding the mine rule from the large database. Decision tree method generally used for the Classification, because it is the simple hierarchical structure for the user understanding {\&} decision making. Various data mining algorithms available for classification based on Artificial Neural Network, Nearest Neighbour Rule {\&} Baysen classifiers but decision tree mining is simple one. ID3 and C4.5 algorithms have been introduced by J.R Quinlan which produce reasonable decision trees. The objective of this paper is to present these algorithms. At first we present the classical algorithm that is ID3, then highlights of this study we will discuss in more detail C4.5 this one is a natural extension of the ID3 algorithm. And we will make a comparison between these two algorithms and others algorithms such as C5.0 and CART.},
author = {Hssina, Badr and Merbouha, Abdelkarim and Ezzikouri, Hanane and Erritali, Mohammed},
doi = {10.14569/SpecialIssue.2014.040203},
file = {:C$\backslash$:/Users/tnkte/Desktop/Paper{\_}3-A{\_}comparative{\_}study{\_}of{\_}decision{\_}tree{\_}ID3{\_}and{\_}C4.5.pdf:pdf},
issn = {2158107X},
journal = {International Journal of Advanced Computer Science and Applications},
keywords = {5 algorithme,c4,classification algorithm,data mining,decision,decision trees are a,id3 algorithme,into groups as,it aims is the,learning,partition of a dataset,supervised,tree,very effective method of},
mendeley-groups = {CSC 791/Reading},
number = {2},
pages = {13--19},
title = {{A comparative study of decision tree ID3 and C4.5}},
year = {2014}
}

@article{Cai2010,
author = {Cai, Yun-lei and Ji, Duo and Cai, Dong-feng},
file = {:C$\backslash$:/Users/tnkte/Desktop/07-NTCIR8-PATMN-CaiY.pdf:pdf},
journal = {Proceedings of the 8th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access},
keywords = {and is a highly,balances the,bm25,efficient,frequency,knn,length of the document,patent classification,snn,word frequency and document},
mendeley-groups = {CSC 791/Reading},
pages = {336--340},
title = {{A KNN Research Paper Classification Method Based on Shared Nearest Neighbor}},
url = {http://research.nii.ac.jp/{~}ntcadm/workshop/OnlineProceedings8/NTCIR/07-NTCIR8-PATMN-CaiY.pdf},
year = {2010}
}

@article{Turney2010,
abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.},
archivePrefix = {arXiv},
arxivId = {1003.1141},
author = {Turney, Peter D. and Pantel, Patrick},
doi = {10.1613/jair.2934},
eprint = {1003.1141},
file = {:C$\backslash$:/Users/tnkte/Desktop/live-2934-4846-jair.pdf:pdf},
isbn = {1076-9757},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
mendeley-groups = {CSC 791/Reading},
pages = {141--188},
title = {{From frequency to meaning: Vector space models of semantics}},
volume = {37},
year = {2010}
}

@article{Tian2014,
abstract = {Bugs are prevalent. To improve software quality, developers often allow users to report bugs that they found using a bug tracking system such as Bugzilla. Users would specify among other things, a description of the bug, the component that is affected by the bug, and the severity of the bug. Based on this information, bug triagers would then assign a priority level to the reported bug. As resources are limited, bug reports would be investigated based on their priority levels. This priority assignment process however is a manual one. Could we do better? In this paper, we propose an automated approach based on machine learning that would recommend a priority level based on information available in bug reports. Our approach considers multiple factors, temporal, textual, author, related-report, severity, and product, that potentially affect the priority level of a bug report. These factors are extracted as features which are then used to train a discriminative model via a new classification algorithm that handles ordinal class labels and imbalanced data. Experiments on more than a hundred thousands bug reports from Eclipse show that we can outperform baseline approaches in terms of average F-measure by a relative improvement of up to 209 {\%}.},
author = {Tian, Yuan and Lo, David and Xia, Xin and Sun, Chengnian},
doi = {10.1007/s10664-014-9331-y},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Bug report management,Multi-factor analysis,Priority prediction},
mendeley-groups = {CSC 791/Reading},
number = {5},
pages = {1354--1383},
title = {{Automated prediction of bug report priority using multi-factor analysis}},
volume = {20},
year = {2014}
}
@article{Klabbankoh2010,
author = {Klabbankoh, Bangorn},
file = {:C$\backslash$:/Users/tnkteja/Desktop/02-drouen.pdf:pdf},
journal = {Computer Communications},
mendeley-groups = {CSC 791/Reading},
title = {{Applied genetic algorithms in information retrieval}},
year = {2010}
}
@article{Bettenburg,
author = {Bettenburg, Nicolas and Zimmermann, Thomas and Kim, Sunghun},
file = {:C$\backslash$:/Users/tnkteja/Desktop/10.1.1.142.8360 (1).pdf:pdf},
mendeley-groups = {CSC 791/Reading},
number = {Section 2},
title = {{Duplicate Bug Reports Considered Harmful . . . Really ?}}
}
@article{Sureka2010,
abstract = {We present an approach to identify duplicate bug reports expressed in free-form text. Duplicate reports needs to be identified to avoid a situation where duplicate reports get assigned to multiple developers. Also, duplicate reports can contain complementary information which can be useful for bug fixing. Automatic identification of duplicate reports (from thousands of existing reports in a bug repository) can increase the productivity of a Triager by reducing the amount of time a Triager spends in searching for duplicate bug reports of any incoming report. The proposed method uses character N-gram-based model for the task of duplicate bug report detection. Previous approaches are word-based whereas this study investigates the usefulness of low-level features based on characters which have certain inherent advantages (such as natural-language independence, robustness towards noisy data and effective handling of domain specific term variations) over word-based features for the problem of duplicate bug report detection. The proposed solution is evaluated on a publicly-available dataset consisting of more than 200 thousand bug reports from the open-source Eclipse project. The dataset consists of ground-truth (pre-annotated dataset having bug reports tagged as duplicate by the Triager). Empirical results and evaluation metrics quantifying retrieval performance indicate that the approach is effective.},
author = {Sureka, Ashish and Jalote, Pankaj},
doi = {10.1109/APSEC.2010.49},
file = {:C$\backslash$:/Users/tnkteja/Desktop/05693213.pdf:pdf},
isbn = {9780769542669},
issn = {15301362},
journal = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
keywords = {Bug report analysis,Duplicate bug detection,Software engineering task automation,Software testing and maintenance,Text classification},
mendeley-groups = {CSC 791/Reading},
pages = {366--374},
title = {{Detecting duplicate bug report using character N-gram-based features}},
year = {2010}
}

@article{Runeson2007,
abstract = {Defect reports are generated from various testing and development activities in software engineering. Sometimes two reports are submitted that describe the same problem, leading to duplicate reports. These reports are mostly written in structured natural language, and as such, it is hard to compare two reports for similarity with formal methods. In order to identify duplicates, we investigate using natural language processing (NLP) techniques to support the identification. A prototype tool is developed and evaluated in a case study analyzing defect reports at Sony Ericsson mobile communications. The evaluation shows that about 2/3 of the duplicates can possibly be found using the NLP techniques. Different variants of the techniques provide only minor result differences, indicating a robust technology. User testing shows that the overall attitude towards the technique is positive and that it has a growth potential.},
author = {Runeson, Per and Alexandersson, Magnus and Nyholm, Oskar},
doi = {10.1109/ICSE.2007.32},
file = {:C$\backslash$:/Users/tnkteja/Desktop/04222611.pdf:pdf},
isbn = {0769528287},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
mendeley-groups = {CSC 791/Reading},
pages = {499--508},
title = {{Detection of duplicate defect reports using natural language processing}},
year = {2007}
}


@article{Xia2013,
abstract = {Bug resolution refers to the activity that developers perform to diagnose, fix, test, and document bugs during software development and maintenance. It is a collaborative activity among developers who contribute their knowledge, ideas, and expertise to resolve bugs. Given a bug report, we would like to recommend the set of bug resolvers that could potentially contribute their knowledge to fix it. We refer to this problem as developer recommendation for bug resolution. In this paper, we propose a new and accurate method named DevRec for the developer recommendation problem. DevRec is a composite method which performs two kinds of analysis: bug reports based analysis (BR-Based analysis), and developer based analysis (D-Based analysis). In the BR-Based analysis, we characterize a new bug report based on past bug reports that are similar to it. Appropriate developers of the new bug report are found by investigating the developers of similar bug reports appearing in the past. In the D-Based analysis, we compute the affinity of each developer to a bug report based on the characteristics of bug reports that have been fixed by the developer before. This affinity is then used to find a set of developers that are “close” to a new bug report. We evaluate our solution on 5 large bug report datasets including GCC, OpenOffice, Mozilla, Netbeans, and Eclipse containing a total of 107,875 bug reports. We show that DevRec could achieve recall@5 and recall@10 scores of 0.4826-0.7989, and 0.6063-0.8924, respectively. We also compare DevRec with other state-of-art methods, such as Bugzie and DREX. The results show that DevRec on average improves recall@5 and recall@10 scores of Bugzie by 57.55{\%} and 39.39{\%} respectively. DevRec also outperforms DREX by improving the average recall@5 and recall@10 scores by 165.38{\%} and 89.36{\%}, respectively.},
author = {Xia, Xin and Lo, David and Wang, Xinyu and Zhou, Bo},
doi = {10.1109/WCRE.2013.6671282},
file = {:C$\backslash$:/Users/tnkteja/Desktop/06671282.pdf:pdf},
isbn = {9781479929313},
issn = {10951350},
journal = {Proceedings - Working Conference on Reverse Engineering, WCRE},
keywords = {Composite Method,Developer Recommendation,Multi-label Learning,Topic Model},
mendeley-groups = {CSC 791/Reading},
pages = {72--81},
title = {{Accurate developer recommendation for bug resolution}},
year = {2013}
}

@article{Wang2008,
abstract = {An open source project typically maintains an open bug repository so that bug reports from all over the world can be gathered. When a new bug report is submitted to the repository, a person, called a triager, examines whether it is a duplicate of an existing bug report. If it is, the triager marks it as duplicate and the bug report is removed from consideration for further work. In the literature, there are approaches exploiting only natural language information to detect duplicate bug reports. In this paper we present a new approach that further involves execution information. In our approach, when a new bug report arrives, its natural language information and execution information are compared with those of the existing bug reports. Then, a small number of existing bug reports are suggested to the triager as the most similar bug reports to the new bug report. Finally, the triager examines the suggested bug reports to determine whether the new bug report duplicates an existing bug report. We calibrated our approach on a subset of the Eclipse bug repository and evaluated our approach on a subset of the Firefox bug repository. The experimental results show that our approach can detect 67{\%}-93{\%} of duplicate bug reports in the Firefox bug repository, compared to 43{\%}-72{\%} using natural language information alone.},
author = {Wang, Xiaoyin Wang Xiaoyin and Zhang, Lu Zhang Lu and Xie, Tao Xie Tao and Anvik, J. and Sun, Jiasu Sun Jiasu},
doi = {10.1145/1368088.1368151},
file = {:C$\backslash$:/Users/tnkteja/Desktop/icse08.pdf:pdf},
isbn = {978-1-60558-079-1},
issn = {0270-5257},
journal = {2008 ACM/IEEE 30th International Conference on Software Engineering},
keywords = {duplicate bug report,execution information,information retrieval},
mendeley-groups = {CSC 791/Reading},
pages = {461--470},
title = {{An approach to detecting duplicate bug reports using natural language and execution information}},
year = {2008}
}

@article{Jalbert2008,
abstract = {Bug tracking systems are important tools that guide the maintenance activities of software developers. The utility of these systems is hampered by an excessive number of duplicate bug reports-in some projects as many as a quarter of all reports are duplicates. Developers must manually identify duplicate bug reports, but this identification process is time-consuming and exacerbates the already high cost of software maintenance. We propose a system that automatically classifies duplicate bug reports as they arrive to save developer time. This system uses surface features, textual semantics, and graph clustering to predict duplicate status. Using a dataset of 29,000 bug reports from the Mozilla project, we perform experiments that include a simulation of a real-time bug reporting environment. Our system is able to reduce development cost by filtering out 8{\%} of duplicate bug reports while allowing at least one report for each real defect to reach developers.},
author = {Jalbert, Nicholas and Weimer, Westley},
doi = {10.1109/DSN.2008.4630070},
file = {:C$\backslash$:/Users/tnkteja/Desktop/weimer-dsn2008.pdf:pdf},
isbn = {978-1-4244-2397-2},
journal = {Proceedings of the International Conference on Dependable Systems and Networks},
mendeley-groups = {CSC 791/Reading},
pages = {52--61},
title = {{Automated duplicate detection for bug tracking systems}},
year = {2008}
}
@article{Hooimeijer2007,
abstract = {Software developers spend a significant portion of their resources handling user-submitted bug reports. For software that is widely deployed, the number of bug reports typically outstrips the resources available to triage them. As a result, some reports may be dealt with too slowly or not at all. We present a descriptive model of bug report quality based on a statistical analysis of surface features of over 27,000 publicly available bug reports for the Mozilla Firefox project. The model predicts whether a bug report is triaged within a given amount of time. Our analysis of this model has implications for bug reporting systems and suggests features that should be emphasized when composing bug reports. We evaluate our model empirically based on its hypothetical performance as an automatic filter of incoming bug reports. Our results show that our model performs significantly better than chance in terms of precision and recall. In addition, we show that our modelcan reduce the overall cost of software maintenance in a setting where the average cost of addressing a bug report is more than 2{\%} of the cost of ignoring an important bug report. Copyright 2007 ACM.},
author = {Hooimeijer, Pieter and Weimer, Westley},
doi = {10.1145/1321631.1321639},
file = {:C$\backslash$:/Users/tnkteja/Desktop/p34-hooimeijer.pdf:pdf},
isbn = {9781595938824},
journal = {Proceedings of the twenty-second IEEE/ACM international conference on Automated software engineering - ASE '07},
keywords = {Automatic filters,Average cost,Computer software,Computer software maintenance,Program debugging,bug report triage,informa-,information retrieval,issue tracking,statistical model},
mendeley-groups = {CSC 791/Reading},
pages = {34},
title = {{Modeling bug report quality}},
url = {http://dl.acm.org/citation.cfm?id=1321631.1321639{\%}5Cnhttp://portal.acm.org/citation.cfm?doid=1321631.1321639{\%}5Cnhttp://www.scopus.com/inward/record.url?eid=2-s2.0-53349117770{\&}partnerID=40{\&}md5=ad7de612065288c0d0be9eb657023d63{\%}5Cnhttp://www.scopus.com/inward/reco},
year = {2007}
}

@article{Hangal2002,
abstract = {This paper introduces DIDUCE, a practical and effective tool that aids programmers in detecting complex program errors and identifying their root causes. By instrumenting a program and observing its behavior as it runs, DIDUCE dynamically formulates hypotheses of invariants obeyed by the program. DIDUCE hypothesizes the strictest invariants at the beginning, and gradually relaxes the hypothesis as violations are detected to allow for new behavior. The violations reported help users to catch software bugs as soon as they occur. They also give programmers new visibility into the behavior of the programs such as identifying rare corner cases in the program logic or even locating hidden errors that corrupt the program's results. We implemented the DIDUCE system for Java programs and applied it to four programs of significant size and complexity. DIDUCE succeeded in identifying the root causes of programming errors in each of the programs quickly and automatically. In particular, DIDUCE is effective in isolating a timing−dependent bug in a released JSSE (Java Secure Socket Extension) library, which would have taken an experienced programmer days to find. Our experience suggests that detecting and checking program invariants dynamically is a simple and effective methodology for debugging many different kinds of program errors across a wide variety of application domains.},
author = {Hangal, Sudheendra and Lam, M.S.},
doi = {10.1109/ICSE.2002.1007976},
file = {:C$\backslash$:/Users/tnkteja/Desktop/Diduce.pdf:pdf},
isbn = {1-58113-472-X},
issn = {02705257},
journal = {Proceedings of the 24th International Conference on Software Engineering. ICSE 2002},
mendeley-groups = {CSC 791/Reading},
pages = {291--301},
title = {{Tracking down software bugs using automatic anomaly detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1007976},
year = {2002}
}

